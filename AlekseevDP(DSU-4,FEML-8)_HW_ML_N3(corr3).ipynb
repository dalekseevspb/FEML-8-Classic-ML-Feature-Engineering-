{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b209837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Алексеев Д.П. (DSU-4,FEML-8)\n",
    "# Домашнее задание к лекции «Функции потерь и оптимизация» (#3). Скорректированное\n",
    "\n",
    "# Задание:\n",
    "# Прочитать про методы оптимизации для нейронных сетей https://habr.com/post/318970/\n",
    "# Реализовать самостоятельно логистическую регрессию\n",
    "\n",
    "# Обучить ее методом градиентного спуска:\n",
    "# -Методом nesterov momentum\n",
    "# -Методом rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8944190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab8daca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.2436244 , 1.4455483 , 3.60483173, 0.10808125, 1.02961383]),\n",
       " array([3.45260891, 3.89623315, 0.23614565, 0.37280601, 3.02422269]),\n",
       " array([11.84507555, 15.57979604,  8.91810042,  2.33458052, 12.13189574]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Зададим произвольную линейную функцию, коэффициенты которой будем подбирать методом градиентного спуска,\n",
    "# используя для этого функцию логистической регрессии\n",
    "N = 100\n",
    "np.random.seed(21)\n",
    "x1 = np.random.uniform(low=0, high=5, size=N)\n",
    "x2 = np.random.uniform(low=0, high=5, size=N)\n",
    "y = 2*x1 + 3*x2 + 1\n",
    "# соответственно, при подборе (в идеале) должны получиться следующие коэффициенты:\n",
    "# params[0] =1 \n",
    "# params[1] при х1 =2\n",
    "# params[2] при х2 =3 \n",
    "\n",
    "x1[:5], x2[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41638d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим функцию логистической кривой (сигмоиды)\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa7ba0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05196425, -0.11119605,  1.0417968 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "lr = 0.0001\n",
    "loss_functions = []\n",
    "params = []\n",
    "probabilities = []\n",
    "np.random.seed(21)\n",
    "params = np.random.normal(size=(3,)) #зададим начальную точку параметров оптимизации (градиентного спуска)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2ed007",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(EPOCHS):\n",
    "    # рассчитаем прогнозную вероятность правильного значения, \n",
    "    # т.е. совпадающего со значением 'y', рассчитанным по формуле y = 2*x1 + 3*x2 + 1\n",
    "    predict_probability = sigmoid(params[0] + params[1] * x1 + params[2] * x2)\n",
    "    probabilities.append(predict_probability)\n",
    "    \n",
    "    # зададим функцию потерь (отклонений прогнозируемых значений от истинных значений 'y')\n",
    "    loss_function = - np.sum(y * np.log(predict_probability) + (1 - y) * np.log(1 - predict_probability)) / N\n",
    "    loss_functions.append(loss_function)\n",
    "    \n",
    "    # уменьшим значения параметров на величину градиента\n",
    "    params[0] -= lr * np.sum((predict_probability - y) / len(probabilities))\n",
    "    params[1] -= lr * np.sum((predict_probability - y) * x1 / len(probabilities))\n",
    "    params[2] -= lr * np.sum((predict_probability - y) * x2 / len(probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6886352c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69582679, 2.01644721, 3.24780056])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params\n",
    "# Вывод: Как видно из результатов градиентного спуска, параметры подобраны не совсем оптимально \n",
    "# (к тому же выдается ошибка деления на ноль, хотя непонятно, \n",
    "# откуда в знаменателе мог взяться ноль - в формуле идёт деление на величину выборки N=100).\n",
    "\n",
    "# Но \"тренд\" в целом определен верно:\n",
    "# y = 2.01644721*x1 + 3.24780056*x2 + 0.69582679\n",
    "\n",
    "# при этом целевая функция была: y = 2*x1 + 3*x2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cda06c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05196425, -0.11119605,  1.0417968 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# из описания на Хабре (https://habr.com/post/318970/),\n",
    "# если я правильно понял, метод Nesterov Momentum -  по сути, то же самое, что обычный градиентный спуск, \n",
    "# но дополнительно нужно запоминать накопленный момент:\n",
    "# «Если мы некоторое время движемся в определённом направлении, то, вероятно, \n",
    "# нам следует туда двигаться некоторое время и в будущем». \n",
    "# Для этого нужно уметь обращаться к недавней истории изменений каждого параметра.\n",
    "\n",
    "# Т.е. нам нужно запоминать предыдущее направление(значение параметров) и умножать на некий коэффициент \"сохранения момента\",\n",
    "# чтобы оптимизироваться быстрее. (В статье на Хабре указано, что \"обычно берётся порядка 0.9\")\n",
    "momentum = 0.9 \n",
    "\n",
    "# задаем переменные для сохранения предыдущих значений свободного члена (t0) и параметров при x1,х2:\n",
    "t0 = 0\n",
    "t1 = 0\n",
    "t2 = 0\n",
    "\n",
    "# уменьшим кол-во эпох вдвое (до 100), остальные параметры оптимизации оставим прежними \n",
    "EPOCHS = 100\n",
    "lr = 0.0001\n",
    "loss_functions_n = []\n",
    "params_n = []\n",
    "probabilities_n = []\n",
    "np.random.seed(21)\n",
    "params_n = np.random.normal(size=(3,)) #зададим начальную точку параметров оптимизации (градиентного спуска)\n",
    "params_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d235d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(EPOCHS):\n",
    "    previous_t0 = t0\n",
    "    previous_t1 = t1\n",
    "    previous_t2 = t2\n",
    "    \n",
    "    # рассчитаем прогнозную вероятность правильного значения, \n",
    "    # т.е. совпадающего со значением 'y', рассчитанным по формуле y = 2*x1 + 3*x2 + 1\n",
    "    predict_probability = sigmoid(params_n[0] + params_n[1] * x1 + params_n[2] * x2)\n",
    "    probabilities_n.append(predict_probability)\n",
    "    \n",
    "    # зададим функцию потерь (отклонений прогнозируемых значений от истинных значений 'y')\n",
    "    loss_function = - np.sum(y * np.log(predict_probability) + (1 - y) * np.log(1 - predict_probability)) / N\n",
    "    loss_functions_n.append(loss_function)\n",
    "    \n",
    "    # вместо обычного вычитания градиента вычтем градиент+накопленный момент \n",
    "    #(коэфф-т сохранения момента, умноженный на предыдущие значения параметров)    \n",
    "    params_n[0] += -(momentum * previous_t0 + lr * np.sum((predict_probability - y) / len(probabilities_n)))\n",
    "    params_n[1] += -(momentum * previous_t1 + lr * np.sum((predict_probability - y) * x1 / len(probabilities_n)))\n",
    "    params_n[2] += -(momentum * previous_t2 + lr * np.sum((predict_probability - y) * x2 / len(probabilities_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ca598d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60822561, 1.76715813, 2.98888303])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_n\n",
    "# Вывод: при вдвое меньшем кол-ве эпох коэфф-ты определены почти так же точно, как при обычном градиентном спуске:\n",
    "# y = 1.76715813*x1 + 2.98888303*x2 + 0.60822561\n",
    "\n",
    "# обычный градиентный спуск(100 эпох): y = 2.01644721*x1 + 3.24780056*x2 + 0.69582679\n",
    "\n",
    "# при этом целевая функция была: y = 2*x1 + 3*x2 + 1\n",
    "\n",
    "# Обращает на себя внимание, что при одинаковом кол-ве эпох = 100 оба метода всё же выдают абсолютно одинаковые коэфф-ты. \n",
    "# Возможно, что отличаются затраты времени на оптимизацию (не замерял).\n",
    "# Видимо, эффективность метода Nesterov Momentum будет более заметна на сложных функциях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf5dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# из описания на Хабре (https://habr.com/post/318970/)\n",
    "# метод RMSProp\n",
    "# def RMSProp(X, gamma, lr=0.25, eps=0.00001):\n",
    "#     Y = []\n",
    "#     EG = 0\n",
    "#     for x in X:\n",
    "#         EG = gamma*EG + (1-gamma)*x*x\n",
    "#         v = lr/np.sqrt(EG + eps)*x\n",
    "#         Y.append(v)\n",
    "#     return np.asarray(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb262dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05196425, -0.11119605,  1.0417968 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gamma - это коэфф-т сохранения момента ('momentum' в методе Nesterov Momentum) \n",
    "gamma = 0.9 \n",
    "\n",
    "# eps - это эпсилон, введённый для исключения ситуации деления на ноль\n",
    "eps=0.00001\n",
    "\n",
    "# задаем переменные для сохранения предыдущих значений свободного члена (t0) и параметров при x1,х2:\n",
    "t0 = 0\n",
    "t1 = 0\n",
    "t2 = 0\n",
    "\n",
    "# остальные параметры оптимизации оставим прежними \n",
    "EPOCHS = 100\n",
    "lr = 0.0001\n",
    "loss_functions_rms = []\n",
    "params_rms = []\n",
    "probabilities_rms = []\n",
    "np.random.seed(21)\n",
    "params_rms = np.random.normal(size=(3,)) #зададим начальную точку параметров оптимизации (градиентного спуска)\n",
    "params_rms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
